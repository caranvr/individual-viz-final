{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing NYC yellow taxi trip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from shapely.geometry import Point, LineString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Extracting trips between 2AM and 3AM on Saturdays and Sundays, and finding number of trips between each O-D pair for each month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize empty list to store each month's trips, which will be concatenated later\n",
    "months = []\n",
    "\n",
    "def get_last_call_trips(month_num:int) -> list:\n",
    "    '''Extracts trips between 2AM and 3AM on Saturdays and Sundays'''\n",
    "    \n",
    "    if month_num < 10:\n",
    "        fname = 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-0' + str(month_num) + '.csv'\n",
    "    else:\n",
    "        fname = 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-' + str(month_num) + '.csv'\n",
    "        \n",
    "    print(f\"Loading dataset {month_num}/12\")\n",
    "    #load full taxi trip dataset for a month\n",
    "    full = pd.read_csv(fname, usecols=['tpep_pickup_datetime', 'PULocationID', 'DOLocationID'],\n",
    "                      parse_dates=['tpep_pickup_datetime'],\n",
    "                      infer_datetime_format=True)\n",
    "    \n",
    "    #set index to pickup datetime\n",
    "    full.set_index('tpep_pickup_datetime', inplace=True)\n",
    "    \n",
    "    #isolate trips that are between 2am and 3am\n",
    "    month = full.between_time('02:00', '03:00').copy()\n",
    "    del(full) #delete full dataset to save memory\n",
    "    \n",
    "    #remove trips within the same zone\n",
    "    month.drop(month[month['PULocationID'] == month['DOLocationID']].index, inplace=True)\n",
    "    \n",
    "    #isolate weekend trips\n",
    "    month['day'] = month.index.dayofweek\n",
    "    month.drop(month[(month['day'] != 5) & (month['day'] != 6)].index, inplace=True)\n",
    "    \n",
    "    #find number of trips for each flow\n",
    "    trips = month.groupby(['PULocationID', 'DOLocationID'], as_index=False).size()\n",
    "    \n",
    "    trips['month'] = month_num\n",
    "    \n",
    "    #add to months list\n",
    "    months.append(trips)\n",
    "    \n",
    "    #delete month dataset\n",
    "    del(month)\n",
    "\n",
    "#Run trip extraction function for each month\n",
    "for i in range(1,13):\n",
    "    get_last_call_trips(i)\n",
    "\n",
    "#Concatenate list of monthly dfs into one df\n",
    "df = pd.concat(months, ignore_index=True)\n",
    "\n",
    "#drop rides that begin or end in zones 264 and 265, which are unknown - 521 trips total\n",
    "df.drop(df[\n",
    "    (df['PULocationID'] == 264) |\n",
    "    (df['PULocationID'] == 265) |\n",
    "    (df['DOLocationID'] == 264) |\n",
    "    (df['DOLocationID'] == 265)\n",
    "].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating a georeferenced flow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones = gpd.read_file('data/taxi_zones/taxi_zones.shp') #downloaded from https://s3.amazonaws.com/nyc-tlc/misc/taxi_zones.zip\n",
    "taxi_zones.set_index('LocationID', inplace=True)\n",
    "\n",
    "#Find centroids of taxi zones (+ coordinates)\n",
    "centroids = taxi_zones['geometry'].centroid\n",
    "\n",
    "#Add coordinates of pickup zone centroid to trip df\n",
    "df = pd.merge(df, centroids.to_frame(), how='left', left_on='PULocationID', right_index=True).rename(columns={0: 'PU_coords'}, inplace=True)\n",
    "\n",
    "#Add coordinates of dropoff zone centroid to trip df\n",
    "df = pd.merge(df, centroids.to_frame(), how='left', left_on='DOLocationID', right_index=True).rename(columns={0: 'DO_coords'}, inplace=True)\n",
    "\n",
    "#Drop 10 records that don't have DO location IDs\n",
    "df.drop(df[df['DO_coords'].isna()].index, inplace=True) \n",
    "\n",
    "#Add new field encoding PU_coords and DO_coords as linestring\n",
    "df['geometry'] = df.apply(lambda x: LineString([x['PU_coords'], x['DO_coords']]), axis=1)\n",
    "\n",
    "#Drop pickup and dropoff centroid columns, as these are no longer necessary\n",
    "df.drop(columns=['PU_coords', 'DO_coords'], inplace=True)\n",
    "\n",
    "#Convert df to geo-df\n",
    "gdf = gpd.GeoDataFrame(df,\n",
    "                geometry='geometry',\n",
    "                crs='epsg:2263')\n",
    "\n",
    "#Reproject to Web Mercator\n",
    "gdf_web_merc = gdf.to_crs('epsg:3857')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Finding area-weighted average income of drop-off locations for each flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load income data for New York state Census tracts\n",
    "income_tract = pd.read_csv('data/ACSST5Y2019.S1903_2021-03-02T112951/ACSST5Y2019.S1903_data_with_overlays_2021-03-02T112740.csv',\n",
    "                          skiprows=[1], usecols=['GEO_ID', 'NAME', 'S1903_C03_001E'])\n",
    "\n",
    "#Select only Census tracts in NYC\n",
    "income_nyc = income_tract.loc[\n",
    "    income_tract['NAME'].str.contains(r'New York County|Kings County|Bronx County|Richmond County|Queens County')\n",
    "].copy()\n",
    "del(income_tract) #save memory by deleting full NY dataset\n",
    "\n",
    "#Drop non-residential Census tracts\n",
    "income_nyc.drop(index=income_nyc[income_nyc['S1903_C03_001E'] == '-'].index, inplace=True)\n",
    "\n",
    "#Convert income column to int\n",
    "income_nyc['S1903_C03_001E'] = income_nyc['S1903_C03_001E'].str.replace('+','').str.replace(',','').astype('int64')\n",
    "\n",
    "#Load shapefile for census tracts\n",
    "tracts_shp = gpd.read_file('https://opendata.arcgis.com/datasets/7bba09631bd740f49ba0442f9603fa38_0.geojson')\n",
    "\n",
    "#Income dataset and tract shapefile have different codes for each tract, but the codes used in the shapefile can be extracted from\n",
    "#the GEO_ID column in the income dataset\n",
    "def get_boroCT(row):\n",
    "    if re.search('Richmond County|New York County', row['NAME']):\n",
    "        return row['GEO_ID'][-7:]\n",
    "    else:\n",
    "        CT_code = row['GEO_ID'][-6:]\n",
    "        if 'Bronx County' in row['NAME']:\n",
    "            return '2' + CT_code\n",
    "        elif 'Queens County' in row['NAME']:\n",
    "            return '4' + CT_code\n",
    "        elif 'Kings County' in row['NAME']:\n",
    "            return '3' + CT_code\n",
    "        \n",
    "income_nyc['BoroCT2010'] = income_nyc.apply(get_boroCT, axis=1)\n",
    "\n",
    "#Join income data to shapefile\n",
    "tracts_income_geo = pd.merge(tracts_shp, income_nyc, on='BoroCT2010', how='left')\n",
    "\n",
    "#Calculate area of each taxi zone\n",
    "taxi_zones['zone_area'] = taxi_zones.area\n",
    "\n",
    "#Reproject Census tracts to projection of taxi zones\n",
    "tracts_income_geo = tracts_income_geo.to_crs('epsg:2263')\n",
    "\n",
    "#Find intersecting polygons between Census tracts and taxi zones\n",
    "tract_zone_inter = gpd.overlay(tracts_income_geo, taxi_zones, how='intersection')\n",
    "\n",
    "#Drop polygons w same geometry\n",
    "tract_zone_inter.drop_duplicates(subset=['geometry'] ,inplace=True)\n",
    "\n",
    "#Find area of each polygon\n",
    "tract_zone_inter['polygon_area'] = tract_zone_inter['geometry'].area\n",
    "\n",
    "#Divide each polygon's area by the area of the taxi zone, then multiply this proportion by the tract's median income\n",
    "tract_zone_inter['pc_zone'] = tract_zone_inter.apply(lambda x: (x['polygon_area']/x['zone_area'])*x['S1903_C03_001E'], axis=1)\n",
    "\n",
    "#Sum income fractions for each taxi zone\n",
    "zone_income = tract_zone_inter.groupby('LocationID').agg({'pc_zone': 'sum'})\n",
    "\n",
    "#Using zone_income, add income of drop-off taxi zones to flow dataset\n",
    "agg_flows_w_income = pd.merge(gdf_web_merc, zone_income, left_on='DOLocationID', right_index=True, how='left').rename(columns={'pc_zone': 'DO_income'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Saving flow dataset as shapefile for use with Mapbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_flows_w_income.to_file('agg_flows_w_income.shp', driver='ESRI Shapefile')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
