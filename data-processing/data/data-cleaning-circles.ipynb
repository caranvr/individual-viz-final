{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from shapely.geometry import Point, LineString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting number of dropoffs per location ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Empty lists for monthly pickups and dropoffs\n",
    "monthly_dropoffs = []\n",
    "monthly_pickups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to prepare monthly pickups and dropoffs\n",
    "def get_last_call_trips(month_num:int, col:str, df_list:list) -> list:\n",
    "    '''Extracts trips between 2AM and 3AM on Saturdays and Sundays\n",
    "    month_num = number between 1 and 12\n",
    "    col = PULocationID or DOLocationID'''\n",
    "    \n",
    "    if month_num < 10:\n",
    "        fname = 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-0' + str(month_num) + '.csv'\n",
    "    else:\n",
    "        fname = 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2020-' + str(month_num) + '.csv'\n",
    "        \n",
    "    print(f\"Loading dataset {month_num}/12\")\n",
    "    #load full taxi trip dataset for a month\n",
    "    full = pd.read_csv(fname, usecols=['tpep_pickup_datetime', 'PULocationID', 'DOLocationID'],\n",
    "                      parse_dates=['tpep_pickup_datetime'],\n",
    "                      infer_datetime_format=True)\n",
    "    \n",
    "    #set index to pickup datetime\n",
    "    full.set_index('tpep_pickup_datetime', inplace=True)\n",
    "    \n",
    "    #isolate trips that are between 2am and 3am\n",
    "    month = full.between_time('02:00', '03:00').copy()\n",
    "    del(full) #delete full dataset to save memory\n",
    "    \n",
    "    #isolate weekend trips\n",
    "    month['day'] = month.index.dayofweek\n",
    "    month.drop(month[(month['day'] != 5) & (month['day'] != 6)].index, inplace=True)\n",
    "    \n",
    "    #find number of pickups or dropoffs for each LocationID\n",
    "    trips = pd.DataFrame(month.groupby(col).size())\n",
    "    \n",
    "    #Add column for month\n",
    "    trips['month'] = month_num\n",
    "    \n",
    "    #add to months list\n",
    "    df_list.append(trips)\n",
    "    \n",
    "    #delete month dataset\n",
    "    del(month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 1/12\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c9482d532ce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Extract dropoffs for each month\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mget_last_call_trips\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DOLocationID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonthly_dropoffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-18d49890eb8e>\u001b[0m in \u001b[0;36mget_last_call_trips\u001b[0;34m(month_num, col, df_list)\u001b[0m\n\u001b[1;32m     14\u001b[0m     full = pd.read_csv(fname, usecols=['tpep_pickup_datetime', 'PULocationID', 'DOLocationID'],\n\u001b[1;32m     15\u001b[0m                       \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tpep_pickup_datetime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                       infer_datetime_format=True)\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#set index to pickup datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/sds2020/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0mamt\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_safe_readinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Extract dropoffs for each month\n",
    "for i in range(1,13):\n",
    "    get_last_call_trips(i, 'DOLocationID', monthly_dropoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in monthly_dropoffs:\n",
    "    mdf = pd.DataFrame(month)\n",
    "    mdf['LocationID'] = mdf.index\n",
    "    mdf.rename(columns={0: 'Dropoffs'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate list of monthly dfs into one df\n",
    "df = pd.concat(monthly_dropoffs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dropoffs = df.pivot_table(values='Dropoffs',\n",
    "              index='LocationID',\n",
    "              columns='month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dropoffs.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_dropoffs.drop(index=[264, 265], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zones = gpd.read_file('taxi_zones/taxi_zones.shp') #downloaded from https://s3.amazonaws.com/nyc-tlc/misc/taxi_zones.zip\n",
    "taxi_zones.set_index('LocationID', inplace=True)\n",
    "\n",
    "#Find centroids of taxi zones (+ coordinates)\n",
    "centroids = taxi_zones['geometry'].centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add coordinates of pickup zone centroid to trip df\n",
    "location_dropoffs = pd.merge(location_dropoffs, centroids.to_frame(), how='left', left_index=True, right_index=True)\n",
    "location_dropoffs.rename(columns={0: 'geometry'}, inplace=True)\n",
    "\n",
    "#Drop 10 records that don't have DO location IDs\n",
    "location_dropoffs.drop(location_dropoffs[location_dropoffs['geometry'].isna()].index, inplace=True) \n",
    "\n",
    "#Convert df to geo-df\n",
    "gdf = gpd.GeoDataFrame(location_dropoffs,\n",
    "                geometry='geometry',\n",
    "                crs='epsg:2263')\n",
    "\n",
    "#Reproject to Web Mercator\n",
    "gdf_web_merc = gdf.to_crs('epsg:3857')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting number of pickups per location ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 1/12\n",
      "Loading dataset 2/12\n",
      "Loading dataset 3/12\n",
      "Loading dataset 4/12\n",
      "Loading dataset 5/12\n",
      "Loading dataset 6/12\n",
      "Loading dataset 7/12\n",
      "Loading dataset 8/12\n",
      "Loading dataset 9/12\n",
      "Loading dataset 10/12\n",
      "Loading dataset 11/12\n",
      "Loading dataset 12/12\n"
     ]
    }
   ],
   "source": [
    "#Extract dropoffs for each month\n",
    "for i in range(1,13):\n",
    "    get_last_call_trips(i, 'PULocationID', monthly_pickups)\n",
    "    \n",
    "for month in monthly_dropoffs:\n",
    "    mdf = pd.DataFrame(month)\n",
    "    mdf['LocationID'] = mdf.index\n",
    "    mdf.rename(columns={0: 'Dropoffs'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load income data for New York state Census tracts\n",
    "income_tract = pd.read_csv('ACSST5Y2019.S1903_2021-03-02T112951/ACSST5Y2019.S1903_data_with_overlays_2021-03-02T112740.csv',\n",
    "                          skiprows=[1], usecols=['GEO_ID', 'NAME', 'S1903_C03_001E'])\n",
    "\n",
    "#Select only Census tracts in NYC\n",
    "income_nyc = income_tract.loc[\n",
    "    income_tract['NAME'].str.contains(r'New York County|Kings County|Bronx County|Richmond County|Queens County')\n",
    "].copy()\n",
    "del(income_tract) #save memory by deleting full NY dataset\n",
    "\n",
    "#Drop non-residential Census tracts\n",
    "income_nyc.drop(index=income_nyc[income_nyc['S1903_C03_001E'] == '-'].index, inplace=True)\n",
    "\n",
    "#Convert income column to int\n",
    "income_nyc['S1903_C03_001E'] = income_nyc['S1903_C03_001E'].str.replace('+','').str.replace(',','').astype('int64')\n",
    "\n",
    "#Load shapefile for census tracts\n",
    "tracts_shp = gpd.read_file('https://opendata.arcgis.com/datasets/7bba09631bd740f49ba0442f9603fa38_0.geojson')\n",
    "\n",
    "#Income dataset and tract shapefile have different codes for each tract, but the codes used in the shapefile can be extracted from\n",
    "#the GEO_ID column in the income dataset\n",
    "def get_boroCT(row):\n",
    "    if re.search('Richmond County|New York County', row['NAME']):\n",
    "        return row['GEO_ID'][-7:]\n",
    "    else:\n",
    "        CT_code = row['GEO_ID'][-6:]\n",
    "        if 'Bronx County' in row['NAME']:\n",
    "            return '2' + CT_code\n",
    "        elif 'Queens County' in row['NAME']:\n",
    "            return '4' + CT_code\n",
    "        elif 'Kings County' in row['NAME']:\n",
    "            return '3' + CT_code\n",
    "        \n",
    "income_nyc['BoroCT2010'] = income_nyc.apply(get_boroCT, axis=1)\n",
    "\n",
    "#Join income data to shapefile\n",
    "tracts_income_geo = pd.merge(tracts_shp, income_nyc, on='BoroCT2010', how='left')\n",
    "\n",
    "#Calculate area of each taxi zone\n",
    "taxi_zones['zone_area'] = taxi_zones.area\n",
    "\n",
    "#Add new column to taxi zone df with LocationID, bc it will be lost in the intersection\n",
    "taxi_zones['zone_ID'] = taxi_zones.index.values\n",
    "\n",
    "#Reproject Census tracts to projection of taxi zones\n",
    "tracts_income_geo = tracts_income_geo.to_crs('epsg:2263')\n",
    "\n",
    "#Find intersecting polygons between Census tracts and taxi zones\n",
    "tract_zone_inter = gpd.overlay(tracts_income_geo, taxi_zones, how='intersection')\n",
    "\n",
    "#Drop polygons w same geometry\n",
    "tract_zone_inter.drop_duplicates(subset=['geometry'] ,inplace=True)\n",
    "\n",
    "#Find area of each polygon\n",
    "tract_zone_inter['polygon_area'] = tract_zone_inter['geometry'].area\n",
    "\n",
    "#Divide each polygon's area by the area of the taxi zone, then multiply this proportion by the tract's median income\n",
    "tract_zone_inter['pc_zone'] = tract_zone_inter.apply(lambda x: (x['polygon_area']/x['zone_area'])*x['S1903_C03_001E'], axis=1)\n",
    "\n",
    "#Sum income fractions for each taxi zone\n",
    "zone_income = tract_zone_inter.groupby('zone_ID').agg({'pc_zone': 'sum'})\n",
    "\n",
    "#Using zone_income, add income of drop-off taxi zones to flow dataset\n",
    "dropoffs_w_income = pd.merge(gdf_web_merc, zone_income, left_index=True, right_index=True, how='left').rename(columns={'pc_zone': 'DO_income'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs_w_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs_w_income.loc[dropoffs_w_income['DO_income'] < 20000, 'DO_income'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthcols = {\n",
    "    1: 'January',\n",
    "    2: 'February',\n",
    "    3: 'March',\n",
    "    4: 'April',\n",
    "    5: 'May',\n",
    "    6: 'June',\n",
    "    7: 'July',\n",
    "    8: 'August',\n",
    "    9: 'September',\n",
    "    10: 'October',\n",
    "    11: 'November',\n",
    "    12: 'December'\n",
    "}\n",
    "\n",
    "dropoffs_w_income.rename(columns=monthcols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in list(months.values()):\n",
    "    dropoffs_w_income[col] = dropoffs_w_income[col].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs_w_income.to_file('dropoffs_w_income.shp', driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs_w_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs_w_income.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs_w_income.plot(column='February',\n",
    "                      scheme='quantiles',\n",
    "                      legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
